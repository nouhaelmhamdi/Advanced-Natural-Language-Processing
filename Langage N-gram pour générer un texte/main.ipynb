{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "14daaf0b",
            "metadata": {},
            "source": [
                "# **Utiliser les Modèles de Langage N-gram pour générer un texte**\n",
                "\n",
                "**Par :** *Nouha EL MHAMDI*\n",
                "\n",
                "**Objectif de l'atelier :**\n",
                "Cet atelier vise à comprendre le principe de la génération de texte à l’aide d’un modèle de langage basé sur les N-grammes. L'objectif est de comprendre comment ce modèle apprend les probabilités des séquences de mots, comment il est entraîné sur un corpus textuel, et comment le choix du paramètre $n$ influence la richesse et la cohérence du texte généré.\n",
                "\n",
                "## **1. Préparation et Corpus**\n",
                "\n",
                "### **Étape 1 : Importation des bibliothèques**\n",
                "\n",
                "Nous importons les bibliothèques nécessaires pour le traitement du langage naturel, les structures de données et les opérations mathématiques.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2c3cd649",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import math\n",
                "import random\n",
                "from collections import Counter, defaultdict"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "940672c7",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Corpus étudié**\n",
                "\n",
                "Le corpus fourni pour l'entraînement du modèle est défini ci-dessous.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d05e16df",
            "metadata": {},
            "outputs": [],
            "source": [
                "Corpus = [\n",
                "    \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux ordinateurs d'apprendre à partir de données.\",\n",
                "    \"Le Maroc est un pays situé en Afrique du Nord. Sa capitale est Rabat. Il possède une riche histoire.\",\n",
                "    \"Rabat est la capitale du Maroc et est connue pour sa culture, son histoire et sa gastronomie.\",\n",
                "    \"L'informatique est la science du traitement automatique de l'information par des machines.\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ac70681",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "## 2. **Nettoyage et Tokenisation**\n",
                "\n",
                "### **Étape 2 : Définition de la fonction de tokenisation**\n",
                "\n",
                "Cette fonction prend une chaîne de caractères, la convertit en minuscules et la divise en tokens (mots) en gérant les caractères spéciaux et les apostrophes.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "2ebaff2d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize(text):\n",
                "    # Convertir en minuscules\n",
                "    text = text.lower()\n",
                "    # Remplacer les caractères non alphabétiques (sauf l'apostrophe) par un espace\n",
                "    text = re.sub(r\"[^a-zàâçéèêëiîïôùûüÿñ\\s']\", ' ', text) \n",
                "    # Remplacer les apostrophes par un espace pour séparer les mots attachés (ex: l'apprentissage -> l apprentissage)\n",
                "    text = re.sub(r\"'\", ' ', text) \n",
                "    \n",
                "    tokens = [t for t in text.split() if t.strip()]\n",
                "    return tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40643640",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Séparation du corpus en phrases**\n",
                "\n",
                "Nous séparons le corpus en phrases en utilisant le point (`.`) comme délimiteur, ce qui est une étape cruciale pour entraîner un modèle de langage sur des séquences de mots au sein d'une même unité sémantique.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "7685a146",
            "metadata": {},
            "outputs": [],
            "source": [
                "sentences = []\n",
                "for doc in Corpus:\n",
                "    for s in doc.split('.'):\n",
                "        s = s.strip()\n",
                "        if len(s) > 0:\n",
                "            sentences.append(s)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd090e08",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Test de l'étape 2**\n",
                "\n",
                "Exécution de la séparation en phrases et test de la fonction de tokenisation.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "c3f9731e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Nombre total de phrases: 6\n",
                        "Liste des phrases:\n",
                        "[\"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux ordinateurs d'apprendre à partir de données\", 'Le Maroc est un pays situé en Afrique du Nord', 'Sa capitale est Rabat', 'Il possède une riche histoire', 'Rabat est la capitale du Maroc et est connue pour sa culture, son histoire et sa gastronomie', \"L'informatique est la science du traitement automatique de l'information par des machines\"]\n",
                        "\n",
                        "Test de tokenisation sur la première phrase:\n",
                        "['l', 'apprentissage', 'automatique', 'est', 'un', 'sous', 'domaine', 'de', 'l', 'intelligence', 'artificielle', 'qui', 'permet', 'aux', 'ordinateurs', 'd', 'apprendre', 'à', 'partir', 'de', 'données']\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Nombre total de phrases: {len(sentences)}\")\n",
                "print(\"Liste des phrases:\")\n",
                "print(sentences)\n",
                "print(\"\\nTest de tokenisation sur la première phrase:\")\n",
                "print(tokenize(sentences[0]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b92e184",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "**Analyse et Interprétation**\n",
                "\n",
                "\n",
                "1.  **Séparation en phrases :** Le résultat indique 6 phrases au total. Cela signifie que le corpus initial, bien que court, a été correctement segmenté en six unités d'entraînement distinctes en utilisant le point comme délimiteur. Chaque phrase sera traitée comme une séquence indépendante pour l'entraînement du modèle.\n",
                "\n",
                "2.  **Nettoyage (`tokenize`) :** Le test sur la première phrase montre que la fonction tokenize a bien séparé les mots. Par exemple, l'expression L'apprentissage est divisée en deux tokens : l et apprentissage. Cette approche simplifiée est courante dans les modèles N-gram pour s'assurer que chaque composant du mot (comme l'article élidé) est compté séparément.\n",
                "## **3. Comptage des N-grammes**\n",
                "\n",
                "### **Étape 3 : Définition des variables globales et de la fonction de comptage**\n",
                "\n",
                "Nous définissons l'ordre du modèle $n$ (initialement $n=2$ pour le Bigramme) et les compteurs pour les N-grammes et les contextes.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "fd88c7d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Définir n (ordre du modèle N-gram)\n",
                "n = 2 \n",
                "\n",
                "# Initialisation des compteurs\n",
                "ngram_counts = Counter()\n",
                "context_counts = Counter()\n",
                "vocab = set()\n",
                "V = 0 # Taille du vocabulaire, sera calculée après le comptage\n",
                "\n",
                "def count_ngrams(n_order):\n",
                "    \"\"\"\n",
                "    Calcule les N-grammes et les contextes pour un ordre n donné.\n",
                "    Met à jour les variables globales ngram_counts, context_counts, vocab et V.\n",
                "    \"\"\"\n",
                "    global n, ngram_counts, context_counts, vocab, V\n",
                "    \n",
                "    n = n_order\n",
                "    ngram_counts.clear()\n",
                "    context_counts.clear()\n",
                "    vocab.clear()\n",
                "    \n",
                "    for s in sentences:\n",
                "        tokens = tokenize(s)\n",
                "        \n",
                "        # Mise à jour du vocabulaire\n",
                "        vocab.update(tokens)\n",
                "        \n",
                "        # Ajout des tokens de début de phrase <s> et de fin de phrase </s>\n",
                "        start_tokens = [\"<s>\"] * (n - 1)\n",
                "        tokens = start_tokens + tokens + [\"</s>\"]\n",
                "        \n",
                "        # Comptage des N-grammes et des contextes\n",
                "        for i in range(len(tokens) - n + 1):\n",
                "            ngram = tuple(tokens[i : i + n])\n",
                "            context = tuple(tokens[i : i + n - 1])\n",
                "            \n",
                "            ngram_counts[ngram] += 1\n",
                "            context_counts[context] += 1\n",
                "\n",
                "    # Taille du vocabulaire (V) = nombre de mots uniques + 1 pour le token de fin de phrase </s>\n",
                "    V = len(vocab) + 1 "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00812b38",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Test de l'étape 3**\n",
                "\n",
                "Exécution du comptage pour $n=2$ et affichage des résultats.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "e8d83caa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ordre du modèle N-gram (n): 2\n",
                        "Taille du vocabulaire (V): 50\n",
                        "\n",
                        "Vocabulaire (sans <s> et </s>): ['afrique', 'apprendre', 'apprentissage', 'artificielle', 'automatique', 'aux', 'capitale', 'connue', 'culture', 'd', 'de', 'des', 'domaine', 'données', 'du', 'en', 'est', 'et', 'gastronomie', 'histoire', 'il', 'information', 'informatique', 'intelligence', 'l', 'la', 'le', 'machines', 'maroc', 'nord', 'ordinateurs', 'par', 'partir', 'pays', 'permet', 'possède', 'pour', 'qui', 'rabat', 'riche', 'sa', 'science', 'situé', 'son', 'sous', 'traitement', 'un', 'une', 'à']\n",
                        "\n",
                        "Nombre total de N-grammes (Top 5):\n",
                        "[(('<s>', 'l'), 2), (('est', 'un'), 2), (('de', 'l'), 2), (('est', 'la'), 2), (('l', 'apprentissage'), 1)]\n",
                        "\n",
                        "Nombre total de contextes (Top 5):\n",
                        "[(('<s>',), 6), (('est',), 6), (('l',), 4), (('de',), 3), (('du',), 3)]\n"
                    ]
                }
            ],
            "source": [
                "# Exécuter le comptage pour n=2\n",
                "count_ngrams(2)\n",
                "\n",
                "print(f\"Ordre du modèle N-gram (n): {n}\")\n",
                "print(f\"Taille du vocabulaire (V): {V}\")\n",
                "print(f\"\\nVocabulaire (sans <s> et </s>): {sorted(list(vocab))}\")\n",
                "print(f\"\\nNombre total de N-grammes (Top 5):\")\n",
                "print(ngram_counts.most_common(5))\n",
                "print(f\"\\nNombre total de contextes (Top 5):\")\n",
                "print(context_counts.most_common(5))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "64facf81",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "**Analyse et Interprétation :**\n",
                "\n",
                "1.  **Préparation :** Le code introduit les marqueurs de début de phrase (`<s>`) et de fin de phrase (`</s>`). Pour un bigramme ($n=2$), chaque phrase commence par un seul `<s>`. Ce modèle fait l'hypothèse que la probabilité d'un mot dépend uniquement du mot qui le précède immédiatement ($P(w_i | w_{i-1})$).\n",
                "\n",
                "\n",
                "2.  **Comptage :** `ngram_counts` stocke la fréquence de chaque séquence de $n$ mots. `context_counts` stocke la fréquence de chaque séquence de $n-1$ mots (le contexte). càd Les Bigrammes les plus fréquents, comme (`<s>`, 'l') (2 occurrences), indiquent les séquences les plus courantes. Le Bigramme (`<s>`, 'l') signifie que deux phrases commencent par le mot l'(par exemple, \"L'apprentissage...\" et \"L'informatique...\").\n",
                "\n",
                "3.  **Vocabulaire ($V$) :** La taille du vocabulaire est de 50. Cette valeur est cruciale. Elle est calculée en prenant les 49 mots uniques du corpus et en ajoutant le token de fin de phrase </s>. Ce $V=50$ sera utilisé dans la formule du lissage de Laplace."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9021e338",
            "metadata": {},
            "source": [
                "## **4. Probabilité Conditionnelle avec Lissage de Laplace**\n",
                "\n",
                "### **Étape 4 : Définition de la fonction de probabilité conditionnelle**\n",
                "\n",
                "Nous définissons la fonction `ngram_prob(context, word)` qui retourne $P(\\text{word} | \\text{context})$ en utilisant le lissage de Laplace avec $\\alpha=1$.\n",
                "\n",
                "**Rôle du Lissage de Laplace :**\n",
                "\n",
                "Le lissage de Laplace (ou *add-one smoothing*) est une technique de régularisation utilisée dans les modèles de langage N-gram pour **éviter le problème de la fréquence zéro**. Il garantit que même les N-grammes jamais vus se voient attribuer une petite probabilité non nulle, empêchant ainsi la probabilité d'une phrase entière de devenir nulle.\n",
                "\n",
                "La formule utilisée est :\n",
                "$$P(w_i | w_{i-n+1}^{i-1}) = \\frac{C(w_{i-n+1}^{i}) + \\alpha}{C(w_{i-n+1}^{i-1}) + \\alpha \\cdot V}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "64f6bbf4",
            "metadata": {},
            "outputs": [],
            "source": [
                "alpha = 1.0 # Paramètre de lissage de Laplace\n",
                "\n",
                "def ngram_prob(context, word):\n",
                "    \"\"\"\n",
                "    Calcule la probabilité conditionnelle P(word | context) avec lissage de Laplace.\n",
                "    \"\"\"\n",
                "    context = tuple(context)\n",
                "    ngram = context + (word,)\n",
                "    \n",
                "    # Numérateur: C(N-gramme) + alpha\n",
                "    num = ngram_counts[ngram] + alpha\n",
                "    \n",
                "    # Dénominateur: C(Contexte) + alpha * V\n",
                "    den = context_counts[context] + alpha * V\n",
                "    \n",
                "    return num / den"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9465666d",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Test de l'étape 4**\n",
                "\n",
                "Exécution de la fonction de probabilité pour différents cas.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "bb93b03d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Probabilité de 'maroc' sachant le contexte ('le',): 0.0392\n",
                        "Probabilité de 'est' sachant le contexte ('le',): 0.0196\n",
                        "Probabilité de 'pomme' (mot inconnu) sachant le contexte ('le',): 0.0196\n"
                    ]
                }
            ],
            "source": [
                "# Assurez-vous que le comptage pour n=2 a été exécuté\n",
                "count_ngrams(2)\n",
                "\n",
                "# Exemple 1: Probabilité d'un N-gramme vu dans le corpus\n",
                "print(f\"Probabilité de 'maroc' sachant le contexte ('le',): {ngram_prob(('le',), 'maroc'):.4f}\")\n",
                "\n",
                "# Exemple 2: Probabilité d'un N-gramme vu dans le corpus\n",
                "print(f\"Probabilité de 'est' sachant le contexte ('le',): {ngram_prob(('le',), 'est'):.4f}\")\n",
                "\n",
                "# Exemple 3: Probabilité d'un mot inconnu ('pomme') - illustre le lissage\n",
                "print(f\"Probabilité de 'pomme' (mot inconnu) sachant le contexte ('le',): {ngram_prob(('le',), 'pomme'):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b17c7e4",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "**Analyse et Interprétation (Étape 4) :**\n",
                "\n",
                "L'analyse des probabilités démontre l'efficacité du lissage de Laplace ($\\alpha=1$).\n",
                "\n",
                "- **Probabilité de 'maroc' sachant 'le' :** Le résultat est **0.0392**. Ce Bigramme `('le', 'maroc')` apparaît 1 fois dans le corpus. La probabilité est calculée en utilisant la formule du lissage : $\\frac{C(\\text{'le', 'maroc'}) + 1}{C(\\text{'le'}) + V} = \\frac{1 + 1}{1 + 50} = \\frac{2}{51}$.\n",
                "\n",
                "- **Probabilité de 'est' sachant 'le' :** Le résultat est **0.0196**. Ce Bigramme `('le', 'est')` n'apparaît **jamais** dans le corpus ($C=0$). La probabilité est donc $\\frac{0 + 1}{1 + 50} = \\frac{1}{51}$.\n",
                "\n",
                "- **Probabilité de 'pomme' (mot inconnu) sachant 'le' :** Le résultat est également **0.0196**. Le mot `pomme` n'est pas dans le vocabulaire, et le Bigramme est donc inconnu.\n",
                "\n",
                "**Conclusion sur le Lissage :** Le lissage de Laplace garantit que même les séquences jamais vues (comme `'le', 'est'`) ou les mots inconnus (`'pomme'`) reçoivent une probabilité non nulle de $\\approx 0.0196$. Sans ce lissage, ces probabilités seraient de zéro, ce qui rendrait la probabilité de toute phrase contenant ces séquences nulle, un problème majeur dans les modèles N-gram.\n",
                "\n",
                "\n",
                "## **5. Probabilités de Transition et Vecteur Initial**\n",
                "\n",
                "### **Étape 5 : Trouver les probabilités de transition**\n",
                "\n",
                "Les probabilités de transition sont les probabilités conditionnelles $P(w_i | w_{i-n+1}^{i-1})$ pour tous les mots $w_i$ du vocabulaire. Nous définissons une fonction pour lister les mots suivants les plus probables pour un contexte donné.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "1393905e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_next(context, topk=5):\n",
                "    \"\"\"\n",
                "    Prédit les top-k mots suivants pour un contexte donné en utilisant ngram_prob.\n",
                "    \"\"\"\n",
                "    if isinstance(context, str):\n",
                "        context = tokenize(context)\n",
                "        \n",
                "    # Tronquer le contexte pour qu'il corresponde à l'ordre n-1\n",
                "    start_tokens = [\"<s>\"] * (n - 1)\n",
                "    context = (start_tokens + context)[-n + 1:] \n",
                "    \n",
                "    candidates = []\n",
                "    # Calculer la probabilité pour chaque mot du vocabulaire (y compris </s>)\n",
                "    full_vocab = vocab.union({\"</s>\"})\n",
                "    \n",
                "    for w in full_vocab:\n",
                "        p = ngram_prob(context, w)\n",
                "        candidates.append((w, p))\n",
                "        \n",
                "    # Trier par probabilité décroissante\n",
                "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    return candidates[:topk]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcf32eac",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Test de l'étape 5**\n",
                "\n",
                "Affichage des probabilités de transition pour deux contextes différents.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "5d0025be",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Probabilités de transition (Top 5) pour le contexte de début de phrase ('<s>',):\n",
                        "[('l', 0.05357142857142857), ('sa', 0.03571428571428571), ('il', 0.03571428571428571), ('le', 0.03571428571428571), ('rabat', 0.03571428571428571)]\n",
                        "\n",
                        "Probabilités de transition (Top 5) pour le contexte ('le',):\n",
                        "[('maroc', 0.0392156862745098), ('connue', 0.0196078431372549), ('information', 0.0196078431372549), ('l', 0.0196078431372549), ('du', 0.0196078431372549)]\n"
                    ]
                }
            ],
            "source": [
                "# Assurez-vous que le comptage pour n=2 a été exécuté\n",
                "count_ngrams(2)\n",
                "\n",
                "print(f\"\\nProbabilités de transition (Top 5) pour le contexte de début de phrase ('<s>',):\")\n",
                "print(predict_next([\"<s>\"]))\n",
                "\n",
                "print(f\"\\nProbabilités de transition (Top 5) pour le contexte ('le',):\")\n",
                "print(predict_next([\"le\"]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "66bf24c2",
            "metadata": {},
            "source": [
                "- **Probabilités de transition pour le contexte ('le',) :** Le mot `maroc` est le plus probable (0.0392) pour suivre `le`, car le Bigramme `('le', 'maroc')` est le seul Bigramme observé dans le corpus qui commence par `le`. Tous les autres mots listés (`connue`, `information`, `l`, `du`) ont la probabilité de base du lissage (0.0196), car ils n'ont jamais été vus directement après `le` dans le corpus.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c288dfac",
            "metadata": {},
            "source": [
                "\n",
                "### **Étape 6 : Trouver le vecteur des probabilités initiales**\n",
                "\n",
                "Le vecteur des probabilités initiales est la distribution de probabilité des mots qui commencent une phrase. Il est obtenu en calculant les probabilités de transition pour le contexte de début de phrase.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "61b9c380",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Vecteur des probabilités initiales (Top 5):\n",
                        "P('l' | '<s>') = 0.0536\n",
                        "P('sa' | '<s>') = 0.0357\n",
                        "P('il' | '<s>') = 0.0357\n",
                        "P('le' | '<s>') = 0.0357\n",
                        "P('rabat' | '<s>') = 0.0357\n"
                    ]
                }
            ],
            "source": [
                "# Le vecteur des probabilités initiales est P(w | <s>)\n",
                "# Nous affichons les 5 plus probables\n",
                "initial_probabilities = predict_next([\"<s>\"], topk=5) \n",
                "\n",
                "print(f\"\\nVecteur des probabilités initiales (Top 5):\")\n",
                "for word, prob in initial_probabilities:\n",
                "    print(f\"P('{word}' | '<s>') = {prob:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71f37e5d",
            "metadata": {},
            "source": [
                "\n",
                "**Analyse et Interprétation :**\n",
                "\n",
                "Ces étapes préparent la phase de génération en identifiant les probabilités de passage d'un état (contexte) à un autre (mot suivant).\n",
                "\n",
                "- **Vecteur des probabilités initiales :** Les mots les plus probables pour commencer une phrase sont `l` (0.0536), `sa`, `il`, `le`, et `rabat` (tous à 0.0357). La probabilité de `l` est plus élevée car il commence deux des six phrases du corpus, tandis que les autres mots n'en commencent qu'une seule. Ce vecteur est utilisé pour choisir le tout premier mot lors de la génération de texte.\n",
                "\n",
                "\n",
                "## **6. Génération de Texte et Impact de $n$**\n",
                "\n",
                "### **Étape 7 : Définition de la fonction de génération de texte**\n",
                "\n",
                "Cette fonction implémente le processus de génération stochastique en utilisant les probabilités conditionnelles calculées. Elle permet de re-entraîner le modèle pour différents ordres $n$.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "26e40e91",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(model_n, max_length=20):\n",
                "    \"\"\"\n",
                "    Génère un texte en utilisant le modèle N-gram entraîné pour l'ordre model_n.\n",
                "    \"\"\"\n",
                "    # Re-entraîner le modèle pour l'ordre n spécifié\n",
                "    local_n = model_n\n",
                "    local_ngram_counts = Counter()\n",
                "    local_context_counts = Counter()\n",
                "    local_vocab = set()\n",
                "\n",
                "    for s in sentences:\n",
                "        tokens = tokenize(s)\n",
                "        local_vocab.update(tokens)\n",
                "        start_tokens = [\"<s>\"] * (local_n - 1)\n",
                "        tokens = start_tokens + tokens + [\"</s>\"]\n",
                "        \n",
                "        for i in range(len(tokens) - local_n + 1):\n",
                "            ngram = tuple(tokens[i : i + local_n])\n",
                "            context = tuple(tokens[i : i + local_n - 1])\n",
                "            local_ngram_counts[ngram] += 1\n",
                "            local_context_counts[context] += 1\n",
                "            \n",
                "    local_V = len(local_vocab) + 1 \n",
                "    local_full_vocab = local_vocab.union({\"</s>\"})\n",
                "    \n",
                "    # Fonction de probabilité locale (pour éviter les dépendances aux globales)\n",
                "    def local_ngram_prob(context, word):\n",
                "        context = tuple(context)\n",
                "        ngram = context + (word,)\n",
                "        num = local_ngram_counts[ngram] + alpha\n",
                "        den = local_context_counts[context] + alpha * local_V\n",
                "        return num / den\n",
                "\n",
                "    # Fonction de prédiction locale\n",
                "    def local_predict_next(context):\n",
                "        if isinstance(context, str):\n",
                "            context = tokenize(context)\n",
                "            \n",
                "        start_tokens = [\"<s>\"] * (local_n - 1)\n",
                "        context = (start_tokens + context)[-local_n + 1:] \n",
                "        \n",
                "        candidates = []\n",
                "        for w in local_full_vocab:\n",
                "            p = local_ngram_prob(context, w)\n",
                "            candidates.append((w, p))\n",
                "            \n",
                "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
                "        return candidates\n",
                "\n",
                "    # Initialisation de la génération\n",
                "    generated_tokens = []\n",
                "    current_context = tuple([\"<s>\"] * (local_n - 1))\n",
                "    \n",
                "    for _ in range(max_length):\n",
                "        candidates = local_predict_next(list(current_context))\n",
                "        \n",
                "        words = [w for w, p in candidates]\n",
                "        probabilities = [p for w, p in candidates]\n",
                "        \n",
                "        total_prob = sum(probabilities)\n",
                "        if total_prob == 0:\n",
                "            break \n",
                "        probabilities = [p / total_prob for p in probabilities]\n",
                "        \n",
                "        # Choix stochastique du mot suivant\n",
                "        next_word = random.choices(words, weights=probabilities, k=1)[0]\n",
                "        \n",
                "        if next_word == \"</s>\":\n",
                "            break\n",
                "            \n",
                "        generated_tokens.append(next_word)\n",
                "        # Mise à jour du contexte: on garde les (n-1) derniers mots\n",
                "        current_context = current_context[1:] + (next_word,)\n",
                "        \n",
                "    return \" \".join(generated_tokens).capitalize()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba1fc36d",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "### **Test de l'étape 7 : Observation de l'effet de $n$**\n",
                "\n",
                "Nous testons la génération de texte pour $n=1$ (Unigramme), $n=2$ (Bigramme) et $n=3$ (Trigramme).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "85ee15cc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Génération de texte (n=1) ---\n",
                        "Modèle Unigramme (n=1): D information informatique de la domaine automatique histoire rabat information sa apprentissage information machines traitement est de un et connue\n",
                        "\n",
                        "--- Génération de texte (n=2) ---\n",
                        "Modèle Bigramme (n=2): Situé domaine qui riche aux qui l apprentissage intelligence une et données maroc en pour est et à intelligence est\n",
                        "\n",
                        "--- Génération de texte (n=3) ---\n",
                        "Modèle Trigramme (n=3): Domaine l de pays du de riche informatique science d d sa informatique nord traitement à apprendre la apprendre d\n"
                    ]
                }
            ],
            "source": [
                "# Fixer la graine pour la reproductibilité des tests\n",
                "random.seed(42) \n",
                "\n",
                "print(\"\\n--- Génération de texte (n=1) ---\")\n",
                "print(f\"Modèle Unigramme (n=1): {generate_text(1)}\")\n",
                "\n",
                "print(\"\\n--- Génération de texte (n=2) ---\")\n",
                "print(f\"Modèle Bigramme (n=2): {generate_text(2)}\")\n",
                "\n",
                "print(\"\\n--- Génération de texte (n=3) ---\")\n",
                "print(f\"Modèle Trigramme (n=3): {generate_text(3)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bfe56568",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "**Analyse et Interprétation :**\n",
                "\n",
                "Les résultats de la génération de texte illustrent le compromis fondamental dans le choix de l'ordre $n$.\n",
                "\n",
                "- **Modèle Unigramme ($n=1$) :** Le texte généré (`D information informatique de la domaine...`) est une suite de mots sans aucune cohérence grammaticale ou sémantique. Cela est normal, car le modèle Unigramme choisit chaque mot indépendamment de son contexte, se basant uniquement sur la fréquence d'apparition du mot dans le corpus.\n",
                "\n",
                "- **Modèle Bigramme ($n=2$) :** Le texte (`Situé domaine qui riche aux qui l apprentissage...`) montre une **cohérence locale** (les paires de mots sont souvent correctes), mais la phrase reste globalement incompréhensible. Le Bigramme capture les dépendances entre deux mots consécutifs, mais échoue à maintenir une structure de phrase sur le long terme.\n",
                "\n",
                "- **Modèle Trigramme ($n=3$) :** Le texte (`Domaine l de pays du de riche informatique science d d sa informatique nord...`) est plus **cohérent** localement, mais on observe un phénomène de **surapprentissage**. Avec un corpus aussi petit, le modèle Trigramme a tendance à reproduire des fragments exacts du texte d'entraînement (`informatique science d`), car il n'a pas assez de données pour généraliser.\n",
                "\n",
                "**Conclusion Pédagogique sur l'Impact de $n$ :**\n",
                "\n",
                "L'ordre $n$ est un hyperparamètre critique.\n",
                "\n",
                "- Un **petit $n$** (comme $n=1$) offre une grande **généralisation** (il peut créer des séquences jamais vues) mais une très faible **cohérence**.\n",
                "- Un **grand $n$** (comme $n=3$) offre une meilleure **cohérence locale** mais souffre de la **rareté des données** et du **surapprentissage**, limitant sa capacité à générer un texte vraiment nouveau.\n",
                "\n",
                "Le choix optimal de $n$ est toujours un compromis entre ces deux aspects. Pour un corpus plus grand, un $n$ plus élevé pourrait être justifié.\n",
                "\n",
                "**Limites du Modèle N-gram :**\n",
                "\n",
                "*   **Problème de la rareté des données :** La croissance exponentielle du nombre de N-grammes avec $n$ rend le modèle impraticable pour de grandes valeurs de $n$.\n",
                "*   **Dépendance limitée :** Le modèle ne peut capturer que les dépendances locales (fenêtre de $n$ mots). Il ne peut pas modéliser les dépendances à longue distance (par exemple, le sujet et le verbe séparés par plusieurs mots).\n",
                "*   **Modèle de Markov :** Il suppose que le mot suivant ne dépend que des $n-1$ mots précédents, ignorant tout le contexte antérieur.\n",
                "\n",
                "Ces limites ont motivé le développement des modèles de langage neuronaux (comme les Transformers) qui sont devenus la norme actuelle en PNL.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
