{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baf3e0b",
   "metadata": {},
   "source": [
    "# **Traitement Automatique du Langage Naturel (TALN) : Modèles de Langage N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c0b28",
   "metadata": {},
   "source": [
    "**Objectif :** Comprendre le principe de la génération de texte à l'aide d'un modèle de langage basé sur les N-grammes, en se concentrant sur l'apprentissage des probabilités, l'entraînement sur un corpus, et l'influence du paramètre $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be93afc",
   "metadata": {},
   "source": [
    "## **Corpus d'Étude**\n",
    "\n",
    "Le corpus utilisé pour l'entraînement de notre modèle est le suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    \"L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui permet aux ordinateurs d'apprendre à partir de données.\",\n",
    "    \"Le Maroc est un pays situé en Afrique du Nord. Sa capitale est Rabat. Il possède une riche histoire.\",\n",
    "    \"Rabat est la capitale du Maroc et est connue pour sa culture, son histoire et sa gastronomie.\",\n",
    "    \"L'informatique est la science du traitement automatique de l'information par des machines.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9b35d",
   "metadata": {},
   "source": [
    "## **1. Importation des Bibliothèques Nécessaires**\n",
    "\n",
    "Nous importons les outils nécessaires pour la tokenisation, la manipulation des N-grammes, le comptage des fréquences et la génération aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a652320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.0 MB/s  0:00:00\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.0 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import sys\n",
    "# Ajout du chemin local pour les bibliothèques installées\n",
    "sys.path.insert(0, '/local_libs') \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df988f66",
   "metadata": {},
   "source": [
    "## **2. Nettoyage et Tokenisation**\n",
    "\n",
    "Cette étape est cruciale pour préparer le texte. Elle consiste à :\n",
    "\n",
    "1. **Nettoyer** la ponctuation en la séparant des mots.\n",
    "2. **Convertir** le texte en minuscules.\n",
    "3. **Tokeniser** (diviser en mots).\n",
    "4. **Ajouter** les marqueurs de début (`<start>`) et de fin de phrase (`<end>`) pour modéliser les probabilités de début et de fin de phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d51e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire (y compris <start> et <end>): 54\n",
      "\n",
      "Extrait du Corpus Tokenisé :\n",
      "[['<start>', 'l', \"'apprentissage\", 'automatique', 'est', 'un', 'sous', '-', 'domaine', 'de', 'l', \"'intelligence\", 'artificielle', 'qui', 'permet', 'aux', 'ordinateurs', 'd', \"'apprendre\", 'à', 'partir', 'de', 'données', '.', '<end>'], ['<start>', 'le', 'maroc', 'est', 'un', 'pays', 'situé', 'en', 'afrique', 'du', 'nord', '.', 'sa', 'capitale', 'est', 'rabat', '.', 'il', 'possède', 'une', 'riche', 'histoire', '.', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    processed_sentences = []\n",
    "    for sentence in corpus:\n",
    "        # Séparation de la ponctuation pour une meilleure tokenisation\n",
    "        sentence = sentence.replace(\"'\", \" '\")\n",
    "        for punct in string.punctuation:\n",
    "            if punct != \"'\":\n",
    "                sentence = sentence.replace(punct, ' ' + punct + ' ')\n",
    "\n",
    "        # Tokenisation en minuscules (langue française)\n",
    "        tokens = word_tokenize(sentence.lower(), language='french')\n",
    "\n",
    "        # Ajout des marqueurs de début et de fin\n",
    "        processed_sentences.append(['<start>'] + tokens + ['<end>'])\n",
    "    return processed_sentences\n",
    "\n",
    "tokenized_corpus = preprocess_corpus(CORPUS)\n",
    "all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
    "vocabulary = set(all_tokens)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(f\"Taille du vocabulaire (y compris <start> et <end>): {vocab_size}\")\n",
    "print(\"\\nExtrait du Corpus Tokenisé :\")\n",
    "print(tokenized_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7f4ac",
   "metadata": {},
   "source": [
    "* **Analyse :**\n",
    "\n",
    "    Ce résultat confirme la bonne exécution de l'étape de prétraitement. Le nombre 54 représente la taille totale de notre vocabulaire, $|V|$, qui est un paramètre essentiel pour le lissage de Laplace. L'extrait du corpus tokenisé montre que chaque phrase a été correctement décomposée en mots (tokens) et mise en minuscules. Surtout, la présence des marqueurs <start> et <end> est validée. Ces marqueurs sont cruciaux pour que le modèle N-gramme puisse apprendre la probabilité qu'un mot soit le premier ou le dernier d'une phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5856aad",
   "metadata": {},
   "source": [
    "## **3. Comptage des N-grammes**\n",
    "\n",
    "Cette fonction compte les occurrences de chaque N-gramme dans le corpus. Le résultat est stocké dans un dictionnaire où la clé est le **contexte** (les $N-1$ mots précédents) et la valeur est un `Counter` des mots qui suivent ce contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1192796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de Comptage Bigramme :\n",
      "Contexte ('le',): Counter({'maroc': 1})\n",
      "Contexte ('<start>',): Counter({'l': 2, 'le': 1, 'rabat': 1})\n"
     ]
    }
   ],
   "source": [
    "def count_ngrams(tokenized_corpus, n):\n",
    "    n_gram_counts = defaultdict(Counter)\n",
    "\n",
    "    for sentence in tokenized_corpus:\n",
    "        # Génération des N-grammes pour la phrase\n",
    "        sentence_ngrams = list(ngrams(sentence, n))\n",
    "\n",
    "        for n_gram in sentence_ngrams:\n",
    "            context = n_gram[:-1]\n",
    "            word = n_gram[-1]\n",
    "            n_gram_counts[context][word] += 1\n",
    "\n",
    "    return n_gram_counts\n",
    "\n",
    "# Exemple de comptage pour N=2 (Bigramme)\n",
    "n_gram_counts_2 = count_ngrams(tokenized_corpus, 2)\n",
    "\n",
    "print(\"Exemple de Comptage Bigramme :\")\n",
    "print(f\"Contexte ('le',): {n_gram_counts_2[('le',)]}\")\n",
    "print(f\"Contexte ('<start>',): {n_gram_counts_2[('<start>',)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897d63b",
   "metadata": {},
   "source": [
    "**Analyse :**\n",
    "\n",
    "Ces résultats affichent les fréquences brutes (les comptes $C(\\cdot)$) des bigrammes dans notre corpus.\n",
    "\n",
    "* Le premier point indique que le mot 'le' n'est suivi par le mot 'maroc' qu'une seule fois ($C(\\text{'le'}, \\text{'maroc'}) = 1$). Le compte total du contexte $C(\\text{'le'})$ est donc de 1.\n",
    "\n",
    "* Le second point montre que le marqueur de début de phrase <start> est suivi par 'l' deux fois, par 'le' une fois, et par 'rabat' une fois. Le compte total du contexte $C(\\text{<start>})$ est donc de $2+1+1=4$, ce qui correspond au nombre total de phrases dans notre corpus.\n",
    "\n",
    "Ces comptes sont la base de l'Estimation du Maximum de Vraisemblance (EMV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a806a8",
   "metadata": {},
   "source": [
    "## **4. Estimer la Probabilité Conditionnelle avec Lissage de Laplace**\n",
    "\n",
    "L'estimation de la probabilité conditionnelle $P(w_{mot} | w_{contexte})$ est effectuée en utilisant l'**Estimation du Maximum de Vraisemblance (EMV)**, ajustée par le **Lissage de Laplace** (ou *Add-one smoothing*).\n",
    "\n",
    "### **Rôle du Lissage de Laplace**\n",
    "\n",
    "Le lissage de Laplace résout le problème de la **rareté des données** (*data sparsity*), où des séquences de mots valides peuvent ne pas apparaître dans le petit corpus d'entraînement. Sans lissage, ces séquences auraient une probabilité de 0, ce qui rendrait impossible la génération de texte les contenant.\n",
    "\n",
    "Le lissage de Laplace réattribue une petite partie de la masse de probabilité à tous les mots du vocabulaire, y compris ceux qui n'ont jamais été vus dans un contexte donné.\n",
    "\n",
    "**Formule de Laplace (avec $\\alpha=1$) :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825168d",
   "metadata": {},
   "source": [
    "$P(w_{mot} | w_{contexte}) = \\frac{C(w_{contexte}, w_{mot}) + \\alpha}{C(w_{contexte}) + \\alpha \\cdot |V|}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0f862",
   "metadata": {},
   "source": [
    "Où :\n",
    "\n",
    "- $C(\\cdot)$ est le compte d'occurrence.\n",
    "- $\\alpha$ est le paramètre de lissage (ici $\\alpha=1$).\n",
    "- $|V|$ est la taille du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94503d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laplace_probability(n_gram_counts, context, word, vocab_size, alpha=1):\n",
    "    # C(context, word)\n",
    "    count_n_gram = n_gram_counts[context].get(word, 0)\n",
    "\n",
    "    # C(context)\n",
    "    count_context = sum(n_gram_counts[context].values())\n",
    "\n",
    "    # Formule de lissage de Laplace\n",
    "    probability = (count_n_gram + alpha) / (count_context + alpha * vocab_size)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ddb6f",
   "metadata": {},
   "source": [
    "## **5. & 6. Probabilités de Transition et Initiales**\n",
    "\n",
    "### **Probabilités de Transition (Étape 5)**\n",
    "\n",
    "Nous calculons les probabilités de transition pour tous les contextes observés dans le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163d25e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de Probabilité de Transition Bigramme (N=2) :\n",
      "P('maroc' | 'le'): 0.0364\n",
      "P('est' | 'le'): 0.0182\n"
     ]
    }
   ],
   "source": [
    "def get_transition_probabilities(n_gram_counts, vocab_size, alpha=1):\n",
    "    transition_probs = defaultdict(dict)\n",
    "\n",
    "    for context, word_counts in n_gram_counts.items():\n",
    "        # Pour chaque contexte, on calcule la probabilité de transition vers chaque mot du vocabulaire\n",
    "        for word in vocabulary:\n",
    "            prob = calculate_laplace_probability(n_gram_counts, context, word, vocab_size, alpha)\n",
    "            transition_probs[context][word] = prob\n",
    "\n",
    "    return transition_probs\n",
    "\n",
    "transition_probs_2 = get_transition_probabilities(n_gram_counts_2, vocab_size)\n",
    "\n",
    "print(\"Exemple de Probabilité de Transition Bigramme (N=2) :\")\n",
    "print(f\"P('maroc' | 'le'): {transition_probs_2[('le',)]['maroc']:.4f}\")\n",
    "print(f\"P('est' | 'le'): {transition_probs_2[('le',)]['est']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80376e62",
   "metadata": {},
   "source": [
    "**Analyse :**\n",
    "\n",
    "Ces valeurs sont les probabilités conditionnelles calculées en appliquant le Lissage de Laplace ($\\alpha=1$) sur les comptes de la cellule précédente.\n",
    "\n",
    "* $P(\\text{'maroc'} | \\text{'le'})$ à 0.0364 : Cette probabilité est calculée comme $\\frac{1 + 1}{1 + 54} = \\frac{2}{55}$. Le numérateur inclut le compte réel (1) plus le lissage (1). Le dénominateur inclut le compte total du contexte (1) plus le lissage appliqué à tout le vocabulaire ($1 \\times 54$).\n",
    "\n",
    "* $P(\\text{'est'} | \\text{'le'})$ à 0.0182 : Cette probabilité est calculée comme $\\frac{0 + 1}{1 + 54} = \\frac{1}{55}$. Le compte réel est zéro ($C(\\text{'le'}, \\text{'est'}) = 0$), car ce bigramme n'a jamais été vu. Le lissage de Laplace lui attribue une probabilité non nulle, garantissant que le modèle ne se bloque pas si la génération tente de suivre 'le' par 'est'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f23d28",
   "metadata": {},
   "source": [
    "### **Probabilités Initiales (Étape 6)**\n",
    "\n",
    "Le vecteur des probabilités initiales correspond à la probabilité qu'un mot $w_1$ soit le premier mot d'une phrase, soit $P(w_1 | <start>)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2280da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('le' | <start>): 0.0345\n"
     ]
    }
   ],
   "source": [
    "def get_initial_probabilities(tokenized_corpus, vocab_size, alpha=1):\n",
    "    # On compte les bigrammes de la forme (<start>, w1)\n",
    "    start_ngrams = count_ngrams(tokenized_corpus, 2)\n",
    "\n",
    "    initial_probs = {}\n",
    "    start_context = ('<start>',)\n",
    "\n",
    "    # On calcule P(w | <start>) pour chaque mot du vocabulaire\n",
    "    for word in vocabulary:\n",
    "        prob = calculate_laplace_probability(start_ngrams, start_context, word, vocab_size, alpha)\n",
    "        initial_probs[word] = prob\n",
    "\n",
    "    return initial_probs\n",
    "\n",
    "initial_probs_2 = get_initial_probabilities(tokenized_corpus, vocab_size)\n",
    "\n",
    "# Exemple de probabilité initiale\n",
    "print(f\"P('le' | <start>): {initial_probs_2['le']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df941f7f",
   "metadata": {},
   "source": [
    "**Analyse :**\n",
    "Cette valeur est la probabilité que le mot 'le' soit le premier mot d'une phrase. Elle est calculée comme $P(\\text{'le'} | \\text{<start>})$ en utilisant le lissage de Laplace sur le contexte <start>.\n",
    "\n",
    "* **Calcul :** $\\frac{C(\\text{<start>}, \\text{'le'}) + 1}{C(\\text{<start>}) + 1 \\cdot |V|} = \\frac{1 + 1}{4 + 54} = \\frac{2}{58}$.\n",
    "\n",
    "* Cette probabilité est utilisée pour choisir le tout premier mot de la phrase générée. Elle montre que le modèle a une légère préférence pour les mots qui ont été vus au début des phrases dans le corpus, tout en laissant une chance à tous les autres mots du vocabulaire de commencer une phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d0db2",
   "metadata": {},
   "source": [
    "\n",
    "| Élément | Analyse |\n",
    "| --- | --- |\n",
    "| **Probabilité Initiale** | C'est la probabilité que le mot `'le'` soit le premier mot d'une phrase, soit $P(\\text{'le'} |\n",
    "| **Calcul :** | $\\frac{C(\\text{<start>}, \\text{'le'}) + 1}{C(\\text{<start>}) + 1 \\cdot |\n",
    "| **Implication** | Cette probabilité est utilisée pour choisir le tout premier mot de la phrase générée. Elle est plus faible que la probabilité de `'l'` (qui serait $\\frac{2+1}{58} \\approx 0.0517$) car `'l'` est apparu deux fois après `<start>` dans le corpus. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f404c0",
   "metadata": {},
   "source": [
    "## **7. Générer un Texte via le Modèle N-gram Entraîné**\n",
    "\n",
    "La génération de texte est un processus itératif :\n",
    "\n",
    "1. Choisir le premier mot en fonction des probabilités initiales.\n",
    "2. Pour chaque mot suivant, déterminer le contexte (les $N-1$ mots précédents).\n",
    "3. Choisir le mot suivant en fonction des probabilités de transition $P(w_{n} | w_{n-N+1:n-1})$.\n",
    "4. Répéter jusqu'à ce que le marqueur `<end>` soit généré ou que la longueur maximale soit atteinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca263ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Texte Généré (N=2 - Bigramme) ---\n",
      "Et aux connue situé rabat 'information, automatique artificielle gastronomie automatique est\n",
      "\n",
      "--- Texte Généré (N=3 - Trigramme) ---\n",
      "'apprentissage sa il d par culture - gastronomie données - il afrique d histoire sous est gastronomie connue est à\n"
     ]
    }
   ],
   "source": [
    "def generate_text(transition_probs, initial_probs, n, max_length=20):\n",
    "\n",
    "    generated_text = []\n",
    "\n",
    "    # 1. Choix du premier mot (basé sur P(w | <start>))\n",
    "    words, probs = zip(*initial_probs.items())\n",
    "    next_word = random.choices(words, weights=probs, k=1)[0]\n",
    "\n",
    "    if next_word == '<end>':\n",
    "        return \"\"\n",
    "\n",
    "    generated_text.append(next_word)\n",
    "\n",
    "    # 2. Génération des mots suivants\n",
    "    for _ in range(max_length - 1):\n",
    "\n",
    "        # Détermination du contexte (les N-1 derniers mots)\n",
    "        if n == 1:\n",
    "            context = ()\n",
    "        else:\n",
    "            context = tuple(generated_text[-(n-1):])\n",
    "\n",
    "        # Récupération des probabilités de transition\n",
    "        current_probs = transition_probs.get(context)\n",
    "\n",
    "        # Si le contexte n'est pas dans les probabilités pré-calculées (cas rare avec Laplace)\n",
    "        if current_probs is None:\n",
    "            # On doit recalculer les probabilités pour ce contexte\n",
    "            n_gram_counts = count_ngrams(tokenized_corpus, n)\n",
    "            all_words = list(vocabulary)\n",
    "            probs = [calculate_laplace_probability(n_gram_counts, context, w, vocab_size) for w in all_words]\n",
    "            next_word = random.choices(all_words, weights=probs, k=1)[0]\n",
    "        else:\n",
    "            words, probs = zip(*current_probs.items())\n",
    "            next_word = random.choices(words, weights=probs, k=1)[0]\n",
    "\n",
    "        if next_word == '<end>':\n",
    "            break\n",
    "\n",
    "        generated_text.append(next_word)\n",
    "\n",
    "    # 3. Nettoyage final du texte\n",
    "    final_text = \" \".join(generated_text)\n",
    "    final_text = final_text.replace(\" ' \", \"'\").replace(\" . \", \". \").replace(\" , \", \", \").replace(\" : \", \": \").replace(\" ! \", \"! \").replace(\" ? \", \"? \").replace(\" ) \", \") \").replace(\" ( \", \" ( \")\n",
    "\n",
    "    return final_text.capitalize()\n",
    "\n",
    "# --- Exécution pour N=2 (Bigramme) ---\n",
    "N_VALUE_2 = 2\n",
    "n_gram_counts_2 = count_ngrams(tokenized_corpus, N_VALUE_2)\n",
    "transition_probs_2 = get_transition_probabilities(n_gram_counts_2, vocab_size)\n",
    "generated_text_2 = generate_text(transition_probs_2, initial_probs_2, N_VALUE_2)\n",
    "\n",
    "# --- Exécution pour N=3 (Trigramme) ---\n",
    "N_VALUE_3 = 3\n",
    "n_gram_counts_3 = count_ngrams(tokenized_corpus, N_VALUE_3)\n",
    "transition_probs_3 = get_transition_probabilities(n_gram_counts_3, vocab_size)\n",
    "generated_text_3 = generate_text(transition_probs_3, initial_probs_2, N_VALUE_3)\n",
    "\n",
    "print(f\"--- Texte Généré (N=2 - Bigramme) ---\")\n",
    "print(generated_text_2)\n",
    "print(f\"\\n--- Texte Généré (N=3 - Trigramme) ---\")\n",
    "print(generated_text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc905a77",
   "metadata": {},
   "source": [
    "**Analyse :**\n",
    "\n",
    "Ces résultats illustrent la limitation du modèle N-gramme sur un petit corpus.\n",
    "\n",
    "* **N=2 (Bigramme) :** Le texte est une mosaïque de paires de mots qui sont souvent grammaticalement correctes localement (par exemple, 'automatique artificielle'), mais qui n'ont aucun sens global. Le modèle ne regarde qu'un mot en arrière, ce qui lui fait perdre toute cohérence structurelle sur le long terme.\n",
    "\n",
    "* **N=3 (Trigramme) :** Le texte est encore plus décousu. Pour un $N$ plus grand, la plupart des séquences de $N$ mots n'existent pas dans le petit corpus. Le modèle est donc contraint d'utiliser le lissage de Laplace pour la majorité des transitions. Cela rend le choix du mot suivant presque aléatoire (probabilité uniforme), ce qui résulte en un texte sans aucune fluidité.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bed3",
   "metadata": {},
   "source": [
    "## **8. Conclusion**\n",
    "\n",
    "L'atelier a permis de mettre en œuvre un modèle de langage N-gramme complet, de l'étape de prétraitement à la génération de texte.\n",
    "\n",
    "### **Influence du Paramètre $N$**\n",
    "\n",
    "| Paramètre $N$ | Caractéristiques | Cohérence |\n",
    "| --- | --- | --- |\n",
    "| **$N=1$ (Unigramme)** | Ne tient compte d'aucun contexte. La génération est purement aléatoire, ne produisant que des listes de mots. | Très faible. |\n",
    "| **$N=2$ (Bigramme)** | Tient compte du mot précédent. La génération produit des paires de mots plausibles, mais la cohérence à long terme est limitée. | Faible à moyenne. |\n",
    "| **$N=3$ (Trigramme)** | Tient compte des deux mots précédents. La génération produit des séquences plus longues et plus cohérentes, souvent des fragments de phrases du corpus. | Moyenne à forte. |\n",
    "\n",
    "**Observation :** Plus $N$ est grand, plus le texte généré est **cohérent** et **proche** des phrases du corpus d'entraînement. Cependant, un $N$ trop grand augmente le risque de **surapprentissage** (*overfitting*) et le problème de la **rareté des données** (nécessitant un lissage plus agressif ou des techniques plus avancées).\n",
    "\n",
    "### **Rôle du Lissage de Laplace**\n",
    "\n",
    "Le lissage de Laplace est essentiel pour garantir que le modèle puisse générer des séquences qui n'étaient pas présentes dans le corpus d'entraînement. En attribuant une probabilité non nulle à tous les N-grammes possibles, il permet au modèle de généraliser et d'éviter les probabilités de transition nulles, ce qui bloquerait la génération.\n",
    "\n",
    "Ce modèle, bien que simple, illustre parfaitement les fondements de la modélisation du langage et les défis liés à la rareté des données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
